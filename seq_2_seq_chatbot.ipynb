{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_2_seq_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhughes/chatbot_sequence2sequence/blob/main/seq_2_seq_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all neccessary resources"
      ],
      "metadata": {
        "id": "stdQiGPZfKUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "e2isktX3fPW5",
        "outputId": "465fc779-5b26-4300-b369-59c6b99b5c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer"
      ],
      "metadata": {
        "id": "4YJAD7EafWuC",
        "outputId": "98103eb8-6fa6-46fa-d8db-7034b4450956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorlayer\n",
            "  Downloading tensorlayer-2.2.3-py3-none-any.whl (363 kB)\n",
            "\u001b[K     |████████████████████████████████| 363 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.13.3)\n",
            "Collecting progressbar2>=3.39.3\n",
            "  Downloading progressbar2-4.0.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.4.1)\n",
            "Requirement already satisfied: h5py>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (2.23.0)\n",
            "Collecting imageio>=2.5.0\n",
            "  Downloading imageio-2.13.5-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9->tensorlayer) (1.5.2)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.39.3->tensorlayer) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (1.24.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (3.0.0)\n",
            "Installing collected packages: pillow, imageio, progressbar2, tensorlayer\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed imageio-2.13.5 pillow-9.0.0 progressbar2-4.0.0 tensorlayer-2.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhvtTEyNfZ7P",
        "outputId": "28600dae-7163-49f2-e29d-26aaf09e1fe6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JY9vACiPfG30"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from tensorlayer.models.seq2seq import Seq2seq\n",
        "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xT9TzTHn6mMs",
        "outputId": "a1c0ce51-62d3-4813-e2c6-167ef2f932be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH='drive/MyDrive/wvh_cl_portfolio_chatbot/'\n",
        "\n",
        "def load_data():\n",
        "    # read data control dictionaries\n",
        "    try:\n",
        "        with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      metadata = None\n",
        "    # read numpy arrays\n",
        "    idx_q = np.load(PATH + 'idx_q.npy')\n",
        "    idx_a = np.load(PATH + 'idx_a.npy')\n",
        "    return metadata, idx_q, idx_a\n",
        "\n",
        "metadata, idx_q, idx_a = load_data()"
      ],
      "metadata": {
        "id": "I0fF-yAYf8i_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " split data into train (70%), test (15%) and valid(15%)\n",
        "    return tuple( (trainX, trainY), (testX,testY), (validX,validY) )\n",
        "'''\n",
        "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
        "    # number of examples\n",
        "    data_len = len(x)\n",
        "    lens = [ int(data_len*item) for item in ratio ]\n",
        "\n",
        "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
        "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
        "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
        "\n",
        "    return (trainX,trainY), (testX,testY), (validX,validY)\n",
        "\n",
        "split_set = split_dataset(idx_q, idx_a)"
      ],
      "metadata": {
        "id": "GmvyqngYhbBm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_setup():\n",
        "    metadata, idx_q, idx_a = load_data()\n",
        "    (trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_q, idx_a)\n",
        "    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n",
        "    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n",
        "    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n",
        "    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n",
        "    validX = tl.prepro.remove_pad_sequences(validX.tolist())\n",
        "    validY = tl.prepro.remove_pad_sequences(validY.tolist())\n",
        "    return metadata, trainX, trainY, testX, testY, validX, validY\n",
        "    \n",
        "#data preprocessing\n",
        "metadata, trainX, trainY, testX, testY, validX, validY = initial_setup()"
      ],
      "metadata": {
        "id": "11r7xXCjhJFI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "src_len = len(trainX)\n",
        "tgt_len = len(trainY)\n",
        "\n",
        "assert src_len == tgt_len"
      ],
      "metadata": {
        "id": "_qF57jfqiSh3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n_step = src_len // batch_size\n",
        "src_vocab_size = len(metadata['idx2w']) # 8002 (0~8001)\n",
        "emb_dim = 1024"
      ],
      "metadata": {
        "id": "KEbF1IDEigvC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = metadata['w2idx']   # dict  word 2 index\n",
        "idx2word = metadata['idx2w']   # list index 2 word\n",
        "unk_id = word2idx['unk']   # 1\n",
        "pad_id = word2idx['_']     # 0"
      ],
      "metadata": {
        "id": "jwTPu80EjN6l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = src_vocab_size  # 8002\n",
        "end_id = src_vocab_size + 1  # 8003"
      ],
      "metadata": {
        "id": "RW4nfwZjjUIw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx.update({'start_id': start_id})\n",
        "word2idx.update({'end_id': end_id})\n",
        "idx2word = idx2word + ['start_id', 'end_id']\n",
        "src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n",
        "num_epochs = 1\n",
        "vocabulary_size = src_vocab_size"
      ],
      "metadata": {
        "id": "F-EVF12XjGCK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def inference(seed, top_n, model):\n",
        "    model.eval()\n",
        "    seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
        "    sentence_id = model(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
        "    sentence = []\n",
        "    for w_id in sentence_id[0]:\n",
        "        w = idx2word[w_id]\n",
        "        if w == 'end_id':\n",
        "            break\n",
        "        sentence = sentence + [w]\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "FGGj61JTjkEE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_seq_length = 20\n",
        "model_ = Seq2seq(\n",
        "        decoder_seq_length = decoder_seq_length,\n",
        "        cell_enc=tf.keras.layers.GRUCell,\n",
        "        cell_dec=tf.keras.layers.GRUCell,\n",
        "        n_layer=3,\n",
        "        n_units=256,\n",
        "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdUEEACtFI8i",
        "outputId": "bb4e17ab-4a35-4844-fe4c-4f9380b5ed40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TL] Embedding embedding_1: (8004, 1024)\n",
            "[TL] RNN rnn_1: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_2: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_3: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_4: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_5: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_6: cell: GRUCell, n_units: 256\n",
            "[TL] Reshape reshape_1\n",
            "[TL] Dense  dense_1: 8004 No Activation\n",
            "[TL] Reshape reshape_2\n",
            "[TL] Reshape reshape_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "model_.train()"
      ],
      "metadata": {
        "id": "hpXSWJL2FLKA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_chatbot(trainX, trainY, model):\n",
        "      for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "        total_loss, n_iter = 0, 0\n",
        "\n",
        "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
        "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
        "\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                ## compute outputs\n",
        "                output = model(inputs = [X, _decode_seqs])\n",
        "                \n",
        "                output = tf.reshape(output, [-1, vocabulary_size])\n",
        "                \n",
        "                ## compute loss and update model\n",
        "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
        "\n",
        "                grad = tape.gradient(loss, model.all_weights)\n",
        "                optimizer.apply_gradients(zip(grad, model.all_weights))\n",
        "            \n",
        "            total_loss += loss\n",
        "            n_iter += 1\n",
        "\n",
        "        # printing average loss after every epoch\n",
        "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
        "      return model"
      ],
      "metadata": {
        "id": "ULC6A5p4FhRX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [\"how are you?\", \"donald trump is terrible\"]"
      ],
      "metadata": {
        "id": "9RbiPNFfFXGZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ = train_chatbot(trainX, trainY, model_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UQTzskOFuJT",
        "outputId": "545fdc87-1acb-426a-97b3-8551719d0249"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1]: loss 5.5761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model):\n",
        "  tl.files.save_npz(model.all_weights, name=PATH + 'model.npz')\n",
        "\n",
        "save_model(model_)"
      ],
      "metadata": {
        "id": "KEqoFWZK7b5L",
        "outputId": "91517b70-0eec-435c-f2be-e601563adb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TL] [*] Saving TL weights into drive/MyDrive/wvh_cl_portfolio_chatbot/model.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TL] [*] Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kkohUOywPEwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}