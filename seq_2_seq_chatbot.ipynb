{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_2_seq_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhughes/chatbot_sequence2sequence/blob/main/seq_2_seq_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all neccessary resources dependencies"
      ],
      "metadata": {
        "id": "stdQiGPZfKUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "e2isktX3fPW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7adb5ec-06d8-485d-f9d5-3afd88feef95"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer"
      ],
      "metadata": {
        "id": "4YJAD7EafWuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa180b76-09a3-4b02-a1f5-f3f99c3e0624"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.13.3)\n",
            "Requirement already satisfied: progressbar2>=3.39.3 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (4.0.0)\n",
            "Requirement already satisfied: h5py>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (3.1.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (0.18.3)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.19.5)\n",
            "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (2.13.5)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer) (1.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9->tensorlayer) (1.5.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.5.0->tensorlayer) (9.0.0)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.39.3->tensorlayer) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->tensorlayer) (1.24.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2021.11.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhvtTEyNfZ7P",
        "outputId": "7b7bac96-8dea-4be1-e20b-73e0773989ac"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "JY9vACiPfG30"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from tensorlayer.models.seq2seq import Seq2seq\n",
        "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT9TzTHn6mMs",
        "outputId": "e12471dc-68ee-438f-8107-f1618ffb9a68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH='drive/MyDrive/wvh_cl_portfolio_chatbot/'\n",
        "\n",
        "def load_data():\n",
        "    # read data control dictionaries\n",
        "    try:\n",
        "        with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      metadata = None\n",
        "    # read numpy arrays\n",
        "    idx_q = np.load(PATH + 'idx_q.npy')\n",
        "    idx_a = np.load(PATH + 'idx_a.npy')\n",
        "    return metadata, idx_q, idx_a\n",
        "\n",
        "metadata, idx_q, idx_a = load_data()"
      ],
      "metadata": {
        "id": "I0fF-yAYf8i_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Function for creating train, test and validation split\n",
        "    Returns the X and Y variables as a tuple( (trainX, trainY), (testX,testY))\n",
        "'''\n",
        "def split_dataset(x, y, ratio = [0.7, 0.3] ):\n",
        "    # number of examples\n",
        "    data_len = len(x)\n",
        "    # get the length for set [length for train, length for test]\n",
        "    lens = [int(data_len * item) for item in ratio]\n",
        "    # create the data splits necessary for training and testing\n",
        "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
        "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
        "\n",
        "    return (trainX,trainY), (testX,testY)"
      ],
      "metadata": {
        "id": "GmvyqngYhbBm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_setup():\n",
        "    metadata, idx_q, idx_a = load_data()\n",
        "    (trainX, trainY), (testX, testY) = split_dataset(idx_q, idx_a)\n",
        "    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n",
        "    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n",
        "    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n",
        "    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n",
        "    return metadata, trainX, trainY, testX, testY\n",
        "    \n",
        "#data preprocessing\n",
        "metadata, trainX, trainY, testX, testY = initial_setup()"
      ],
      "metadata": {
        "id": "11r7xXCjhJFI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "src_len = len(trainX)\n",
        "tgt_len = len(trainY)\n",
        "\n",
        "assert src_len == tgt_len"
      ],
      "metadata": {
        "id": "_qF57jfqiSh3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n_step = src_len\n",
        "src_vocab_size = len(metadata['idx2w'])\n",
        "emb_dim = 1024"
      ],
      "metadata": {
        "id": "KEbF1IDEigvC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = metadata['w2idx']   # dict  word 2 index\n",
        "idx2word = metadata['idx2w']   # list index 2 word\n",
        "unk_id = word2idx['unk']   # 1\n",
        "pad_id = word2idx['_']"
      ],
      "metadata": {
        "id": "jwTPu80EjN6l"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = src_vocab_size \n",
        "end_id = src_vocab_size + 1 "
      ],
      "metadata": {
        "id": "RW4nfwZjjUIw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx.update({'start_id': start_id})\n",
        "word2idx.update({'end_id': end_id})\n",
        "idx2word = idx2word + ['start_id', 'end_id']\n",
        "src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n",
        "num_epochs = 50\n",
        "vocabulary_size = src_vocab_size"
      ],
      "metadata": {
        "id": "F-EVF12XjGCK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_seq_length = 20\n",
        "model_ = Seq2seq(\n",
        "        decoder_seq_length = decoder_seq_length,\n",
        "        cell_enc=tf.keras.layers.GRUCell,\n",
        "        cell_dec=tf.keras.layers.GRUCell,\n",
        "        n_layer=3,\n",
        "        n_units=256,\n",
        "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdUEEACtFI8i",
        "outputId": "fb0e722c-2473-4bdc-d284-4f779e34f34a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TL] Embedding embedding_6: (8008, 1024)\n",
            "[TL] RNN rnn_31: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_32: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_33: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_34: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_35: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_36: cell: GRUCell, n_units: 256\n",
            "[TL] Reshape reshape_16\n",
            "[TL] Dense  dense_6: 8008 No Activation\n",
            "[TL] Reshape reshape_17\n",
            "[TL] Reshape reshape_18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_chatbot(trainX, trainY, model):\n",
        "      for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "        total_loss, n_iter = 0, 0\n",
        "\n",
        "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
        "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
        "\n",
        "            # remove all padding (reduce sequential zeros to one zero)\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            # create target encoded sequences for masking\n",
        "            target_sequences = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
        "            target_sequences = tl.prepro.pad_sequences(target_sequences, maxlen=decoder_seq_length)\n",
        "            # create decode sequences which realte to the encoded ones\n",
        "            decode_sequences = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
        "            decode_sequences = tl.prepro.pad_sequences(decode_sequences, maxlen=decoder_seq_length)\n",
        "            # Generate a mask for a set of sequences\n",
        "            target_mask = tl.prepro.sequences_get_mask(target_sequences)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                ## compute outputs\n",
        "                output = model(inputs = [X, decode_sequences])\n",
        "                \n",
        "                # reshape output into  chape for caluclating the loss\n",
        "                output = tf.reshape(output, [-1, vocabulary_size])\n",
        "\n",
        "                ## compute loss and update model\n",
        "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_sequences, input_mask=target_mask)\n",
        "\n",
        "                grad = tape.gradient(loss, model.all_weights)\n",
        "                optimizer.apply_gradients(zip(grad, model.all_weights))\n",
        "            \n",
        "            total_loss += loss\n",
        "            n_iter += 1\n",
        "\n",
        "        # printing average loss after every epoch\n",
        "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
        "      return model"
      ],
      "metadata": {
        "id": "ULC6A5p4FhRX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ = train_chatbot(trainX, trainY, model_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UQTzskOFuJT",
        "outputId": "edd63170-7268-47ad-c624-e49796a9ce34"
      },
      "execution_count": 30,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/16]: loss 5.6222\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/16]: loss 4.9761\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/16]: loss 4.7179\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/16]: loss 4.5122\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/16]: loss 4.3225\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/16]: loss 4.1413\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/16]: loss 3.9706\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/16]: loss 3.8097\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/16]: loss 3.6613\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/16]: loss 3.5287\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/16]: loss 3.4112\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/16]: loss 3.3088\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/16]: loss 3.2179\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/16]: loss 3.1417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/16]: loss 3.0753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/16]: loss 3.0166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(seed, top_n, model):\n",
        "    model.eval()\n",
        "    seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
        "    sentence_id = model(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
        "    sentence = []\n",
        "    for w_id in sentence_id[0]:\n",
        "        w = idx2word[w_id]\n",
        "        if w == 'end_id':\n",
        "            break\n",
        "        sentence = sentence + [w]\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "FGGj61JTjkEE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model):\n",
        "  tl.files.save_npz(model.all_weights, name=PATH + 'model.npz')\n",
        "\n",
        "save_model(model_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEqoFWZK7b5L",
        "outputId": "f56eac55-b675-4799-d3bb-367685a85a63"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TL] [*] Saving TL weights into drive/MyDrive/wvh_cl_portfolio_chatbot/model.npz\n",
            "[TL] [*] Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "model_.train()"
      ],
      "metadata": {
        "id": "hpXSWJL2FLKA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights = tl.files.load_npz(name=PATH + 'model.npz')\n",
        "tl.files.assign_weights(load_weights, model_)"
      ],
      "metadata": {
        "id": "XP8zxFJ1VPc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [\"who do you think you are?\"]\n",
        "# Take the 3 generated respones from the model\n",
        "for seed in seeds:\n",
        "    print(\"Query >\", seed)\n",
        "    top_n = 3\n",
        "    for i in range(top_n):\n",
        "        sentence = inference(seed, top_n, model_)\n",
        "        print(\" >\", ' '.join(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkohUOywPEwj",
        "outputId": "8dd73066-f041-4f0f-e136-328671bdc00b"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query > who do you think you are?\n",
            " > monica you can do it tomorrow\n",
            " > monica because i won the debate\n",
            " > monica and i think paul is a big boy\n"
          ]
        }
      ]
    }
  ]
}